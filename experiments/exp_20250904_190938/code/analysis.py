"""
Statistical analysis and visualization for DERP-VAE experiment results
"""
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import pingouin as pg
from pathlib import Path
from typing import Dict, List, Tuple
import logging

logger = logging.getLogger(__name__)

def load_experiment_results(results_path: str) -> pd.DataFrame:
    """Load and process experiment results into DataFrame"""
    results_path = Path(results_path)
    
    # Load combined results
    with open(results_path / 'comparative_results.json', 'r') as f:
        results = json.load(f)
    
    # Process results into DataFrame
    processed_data = []
    
    for result in results:
        exp_name = result['experiment_name']
        model_type = result['model_type']
        
        # Extract final metrics (last test evaluation)
        if result['test_metrics']:
            final_metrics = result['test_metrics'][-1]
            
            row = {\n                'experiment_name': exp_name,\n                'model_type': model_type,\n                'training_time': result['training_time'],\n                'beta': result['beta'],\n                **final_metrics\n            }\n            \n            # Add model-specific configurations\n            config = result['model_config']\n            if 'n_probes' in config:\n                row['n_probes'] = config['n_probes']\n                row['enforcement_weight'] = config['enforcement_weight']\n            else:\n                row['n_probes'] = 0\n                row['enforcement_weight'] = 0.0\n            \n            processed_data.append(row)\n    \n    return pd.DataFrame(processed_data)\n\ndef statistical_comparison(df: pd.DataFrame, metric: str) -> Dict:\n    \"\"\"Perform statistical comparisons between model types\"\"\"\n    logger.info(f\"Performing statistical analysis for {metric}\")\n    \n    # Separate DERP-VAE and baselines\n    derp_data = df[df['model_type'] == 'derp'][metric].values\n    standard_data = df[df['model_type'] == 'standard'][metric].values\n    \n    results = {}\n    \n    # T-test between DERP-VAE and Standard VAE\n    if len(derp_data) > 1 and len(standard_data) > 1:\n        t_stat, p_value = stats.ttest_ind(derp_data, standard_data)\n        results['ttest'] = {\n            't_statistic': float(t_stat),\n            'p_value': float(p_value),\n            'significant': p_value < 0.05\n        }\n    \n    # Effect size (Cohen's d)\n    pooled_std = np.sqrt((np.var(derp_data, ddof=1) + np.var(standard_data, ddof=1)) / 2)\n    if pooled_std > 0:\n        cohens_d = (np.mean(derp_data) - np.mean(standard_data)) / pooled_std\n        results['effect_size'] = {\n            'cohens_d': float(cohens_d),\n            'interpretation': interpret_cohens_d(cohens_d)\n        }\n    \n    # Descriptive statistics\n    results['descriptives'] = {\n        'derp_mean': float(np.mean(derp_data)) if len(derp_data) > 0 else None,\n        'derp_std': float(np.std(derp_data)) if len(derp_data) > 0 else None,\n        'standard_mean': float(np.mean(standard_data)) if len(standard_data) > 0 else None,\n        'standard_std': float(np.std(standard_data)) if len(standard_data) > 0 else None,\n        'improvement': float((np.mean(derp_data) - np.mean(standard_data)) / np.mean(standard_data) * 100) if len(standard_data) > 0 and np.mean(standard_data) != 0 else None\n    }\n    \n    return results\n\ndef interpret_cohens_d(d: float) -> str:\n    \"\"\"Interpret Cohen's d effect size\"\"\"\n    abs_d = abs(d)\n    if abs_d < 0.2:\n        return \"negligible\"\n    elif abs_d < 0.5:\n        return \"small\"\n    elif abs_d < 0.8:\n        return \"medium\"\n    else:\n        return \"large\"\n\ndef hypothesis_testing(df: pd.DataFrame) -> Dict:\n    \"\"\"Test research hypotheses H1 and H2\"\"\"\n    logger.info(\"Testing research hypotheses\")\n    \n    results = {}\n    \n    # H1: Active enforcement improves model performance and robustness\n    # Metrics: Lower KL divergence, higher mutual information, better reconstruction\n    \n    # H2: Posterior collapse is an identifiability problem\n    # Metrics: Higher activation rate, more stable training\n    \n    key_metrics = [\n        'kl_divergence',  # Lower is better (less collapse)\n        'mutual_information',  # Higher is better\n        'activation_rate',  # Higher is better\n        'recon_loss',  # Lower is better\n        'normality_compliance',  # Higher is better\n        'training_time'  # Efficiency metric\n    ]\n    \n    for metric in key_metrics:\n        if metric in df.columns:\n            results[metric] = statistical_comparison(df, metric)\n    \n    # Overall hypothesis assessment\n    results['hypothesis_assessment'] = assess_hypotheses(results)\n    \n    return results\n\ndef assess_hypotheses(statistical_results: Dict) -> Dict:\n    \"\"\"Assess whether hypotheses H1 and H2 are supported by the data\"\"\"\n    assessment = {\n        'H1_supported': False,\n        'H2_supported': False,\n        'evidence_summary': {}\n    }\n    \n    # H1: Active enforcement improves performance\n    h1_evidence = []\n    \n    # Check KL divergence reduction (collapse prevention)\n    if 'kl_divergence' in statistical_results:\n        kl_results = statistical_results['kl_divergence']\n        if 'descriptives' in kl_results and kl_results['descriptives']['improvement'] is not None:\n            improvement = kl_results['descriptives']['improvement']\n            if improvement < -50:  # >50% reduction target\n                h1_evidence.append(f\"KL divergence reduced by {abs(improvement):.1f}% (target: >50%)\")\n                assessment['H1_supported'] = True\n            else:\n                h1_evidence.append(f\"KL divergence reduction {abs(improvement):.1f}% < 50% target\")\n    \n    # Check mutual information increase\n    if 'mutual_information' in statistical_results:\n        mi_results = statistical_results['mutual_information']\n        if 'descriptives' in mi_results and mi_results['descriptives']['improvement'] is not None:\n            improvement = mi_results['descriptives']['improvement']\n            if improvement > 0:\n                h1_evidence.append(f\"Mutual information increased by {improvement:.1f}%\")\n    \n    # H2: Better identifiability through active enforcement\n    h2_evidence = []\n    \n    # Check activation rate improvement\n    if 'activation_rate' in statistical_results:\n        ar_results = statistical_results['activation_rate']\n        if 'descriptives' in ar_results and ar_results['descriptives']['improvement'] is not None:\n            improvement = ar_results['descriptives']['improvement']\n            if improvement > 0:\n                h2_evidence.append(f\"Activation rate improved by {improvement:.1f}%\")\n                assessment['H2_supported'] = True\n    \n    # Check reconstruction quality maintenance\n    if 'recon_loss' in statistical_results:\n        recon_results = statistical_results['recon_loss']\n        if 'descriptives' in recon_results and recon_results['descriptives']['improvement'] is not None:\n            degradation = recon_results['descriptives']['improvement']\n            if degradation < 10:  # <10% degradation target\n                h2_evidence.append(f\"Reconstruction degradation {degradation:.1f}% < 10% target\")\n    \n    assessment['evidence_summary'] = {\n        'H1_evidence': h1_evidence,\n        'H2_evidence': h2_evidence\n    }\n    \n    return assessment\n\ndef create_visualizations(df: pd.DataFrame, output_path: str):\n    \"\"\"Create comprehensive visualizations of experimental results\"\"\"\n    output_path = Path(output_path)\n    output_path.mkdir(parents=True, exist_ok=True)\n    \n    # Set style\n    plt.style.use('seaborn-v0_8')\n    sns.set_palette(\"husl\")\n    \n    # 1. Main comparison plot: Posterior collapse metrics\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    fig.suptitle('DERP-VAE vs Baselines: Posterior Collapse Prevention', fontsize=16, fontweight='bold')\n    \n    # KL Divergence (collapse metric)\n    ax1 = axes[0, 0]\n    sns.barplot(data=df, x='experiment_name', y='kl_divergence', ax=ax1)\n    ax1.set_title('KL Divergence to Prior (Lower = Less Collapse)')\n    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n    ax1.axhline(y=df['kl_divergence'].mean(), color='red', linestyle='--', alpha=0.7, label='Mean')\n    \n    # Mutual Information\n    ax2 = axes[0, 1] \n    sns.barplot(data=df, x='experiment_name', y='mutual_information', ax=ax2)\n    ax2.set_title('Mutual Information I(x,z) (Higher = Better)')\n    ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n    \n    # Activation Rate\n    ax3 = axes[1, 0]\n    sns.barplot(data=df, x='experiment_name', y='activation_rate', ax=ax3)\n    ax3.set_title('Latent Dimension Activation Rate (Higher = Better)')\n    ax3.set_xticklabels(ax3.get_xticklabels(), rotation=45, ha='right')\n    \n    # Reconstruction Loss\n    ax4 = axes[1, 1]\n    sns.barplot(data=df, x='experiment_name', y='recon_loss', ax=ax4)\n    ax4.set_title('Reconstruction Loss (Lower = Better)')\n    ax4.set_xticklabels(ax4.get_xticklabels(), rotation=45, ha='right')\n    \n    plt.tight_layout()\n    plt.savefig(output_path / 'main_comparison.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    # 2. Statistical significance plot\n    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n    \n    # Create comparison between DERP-VAE and baselines\n    derp_metrics = df[df['model_type'] == 'derp'][['kl_divergence', 'mutual_information', 'activation_rate']].mean()\n    standard_metrics = df[df['model_type'] == 'standard'][['kl_divergence', 'mutual_information', 'activation_rate']].mean()\n    \n    metrics = ['KL Divergence', 'Mutual Information', 'Activation Rate']\n    derp_values = [derp_metrics['kl_divergence'], derp_metrics['mutual_information'], derp_metrics['activation_rate']]\n    standard_values = [standard_metrics['kl_divergence'], standard_metrics['mutual_information'], standard_metrics['activation_rate']]\n    \n    x = np.arange(len(metrics))\n    width = 0.35\n    \n    ax.bar(x - width/2, standard_values, width, label='Standard VAE', alpha=0.8)\n    ax.bar(x + width/2, derp_values, width, label='DERP-VAE', alpha=0.8)\n    \n    ax.set_xlabel('Metrics')\n    ax.set_ylabel('Values')\n    ax.set_title('DERP-VAE vs Standard VAE: Key Metrics Comparison')\n    ax.set_xticks(x)\n    ax.set_xticklabels(metrics)\n    ax.legend()\n    \n    plt.tight_layout()\n    plt.savefig(output_path / 'statistical_comparison.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    # 3. Distributional compliance\n    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n    sns.barplot(data=df, x='experiment_name', y='normality_compliance', ax=ax)\n    ax.set_title('Distributional Compliance (Normality Test Pass Rate)')\n    ax.set_ylabel('Fraction of Dimensions Passing Normality Tests')\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n    ax.axhline(y=0.95, color='green', linestyle='--', alpha=0.7, label='Target (95%)')\n    ax.legend()\n    \n    plt.tight_layout()\n    plt.savefig(output_path / 'distributional_compliance.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    # 4. Efficiency analysis\n    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n    \n    # Training time comparison\n    sns.scatterplot(data=df, x='training_time', y='kl_divergence', \n                   hue='model_type', style='model_type', s=100, ax=ax)\n    ax.set_xlabel('Training Time (seconds)')\n    ax.set_ylabel('KL Divergence (Posterior Collapse)')\n    ax.set_title('Efficiency vs Performance Trade-off')\n    \n    # Add experiment labels\n    for idx, row in df.iterrows():\n        ax.annotate(row['experiment_name'], \n                   (row['training_time'], row['kl_divergence']),\n                   xytext=(5, 5), textcoords='offset points', \n                   fontsize=8, alpha=0.7)\n    \n    plt.tight_layout()\n    plt.savefig(output_path / 'efficiency_analysis.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    logger.info(f\"Visualizations saved to {output_path}\")\n\ndef generate_report(df: pd.DataFrame, statistical_results: Dict, output_path: str):\n    \"\"\"Generate comprehensive experimental report\"\"\"\n    report = []\n    report.append(\"# DERP-VAE Experimental Results Report\\n\")\n    report.append(\"## Experiment Overview\\n\")\n    report.append(\"**Hypothesis Testing**: Distribution Enforcement via Random Probe (DERP) prevents VAE posterior collapse\\n\")\n    report.append(f\"**Total Experiments**: {len(df)}\\n\")\n    report.append(f\"**Models Tested**: {', '.join(df['experiment_name'].unique())}\\n\")\n    \n    # Hypothesis assessment\n    hypothesis_results = statistical_results.get('hypothesis_assessment', {})\n    report.append(\"\\n## Hypothesis Testing Results\\n\")\n    \n    h1_supported = hypothesis_results.get('H1_supported', False)\n    h2_supported = hypothesis_results.get('H2_supported', False)\n    \n    report.append(f\"**H1 (Active Enforcement Improves Performance)**: {'✓ SUPPORTED' if h1_supported else '✗ NOT SUPPORTED'}\\n\")\n    report.append(f\"**H2 (Identifiability Problem Resolution)**: {'✓ SUPPORTED' if h2_supported else '✗ NOT SUPPORTED'}\\n\")\n    \n    # Evidence summary\n    evidence = hypothesis_results.get('evidence_summary', {})\n    if 'H1_evidence' in evidence:\n        report.append(\"\\n### H1 Evidence:\\n\")\n        for item in evidence['H1_evidence']:\n            report.append(f\"- {item}\\n\")\n    \n    if 'H2_evidence' in evidence:\n        report.append(\"\\n### H2 Evidence:\\n\")\n        for item in evidence['H2_evidence']:\n            report.append(f\"- {item}\\n\")\n    \n    # Key findings\n    report.append(\"\\n## Key Findings\\n\")\n    \n    # Best DERP-VAE vs Best Standard VAE\n    derp_best = df[df['model_type'] == 'derp'].loc[df[df['model_type'] == 'derp']['kl_divergence'].idxmin()]\n    standard_best = df[df['model_type'] == 'standard'].loc[df[df['model_type'] == 'standard']['kl_divergence'].idxmin()]\n    \n    kl_improvement = (standard_best['kl_divergence'] - derp_best['kl_divergence']) / standard_best['kl_divergence'] * 100\n    report.append(f\"1. **Best DERP-VAE** ({derp_best['experiment_name']}) achieved {kl_improvement:.1f}% reduction in posterior collapse vs best baseline\\n\")\n    \n    time_overhead = (derp_best['training_time'] - standard_best['training_time']) / standard_best['training_time'] * 100\n    report.append(f\"2. **Computational Overhead**: {time_overhead:.1f}% increase in training time\\n\")\n    \n    activation_improvement = (derp_best['activation_rate'] - standard_best['activation_rate']) / standard_best['activation_rate'] * 100\n    report.append(f\"3. **Identifiability**: {activation_improvement:.1f}% improvement in latent dimension utilization\\n\")\n    \n    # Statistical significance\n    report.append(\"\\n## Statistical Analysis\\n\")\n    \n    for metric, results in statistical_results.items():\n        if metric != 'hypothesis_assessment' and 'ttest' in results:\n            ttest = results['ttest']\n            effect = results.get('effect_size', {})\n            \n            report.append(f\"\\n### {metric.replace('_', ' ').title()}\\n\")\n            report.append(f\"- **Statistical Significance**: {'Yes' if ttest['significant'] else 'No'} (p={ttest['p_value']:.4f})\\n\")\n            if 'cohens_d' in effect:\n                report.append(f\"- **Effect Size**: {effect['cohens_d']:.3f} ({effect['interpretation']})\\n\")\n    \n    # Performance table\n    report.append(\"\\n## Detailed Results Table\\n\")\n    report.append(\"| Experiment | KL Div | Mutual Info | Activation Rate | Recon Loss | Training Time |\\n\")\n    report.append(\"|------------|--------|-------------|-----------------|------------|---------------|\\n\")\n    \n    for _, row in df.iterrows():\n        report.append(f\"| {row['experiment_name']} | {row['kl_divergence']:.4f} | {row['mutual_information']:.4f} | {row['activation_rate']:.4f} | {row['recon_loss']:.4f} | {row['training_time']:.1f}s |\\n\")\n    \n    # Conclusions\n    report.append(\"\\n## Conclusions\\n\")\n    \n    if h1_supported and h2_supported:\n        report.append(\"Both research hypotheses are **SUPPORTED** by the experimental evidence. \")\n        report.append(\"DERP-VAE successfully prevents posterior collapse through active distributional enforcement.\\n\")\n    elif h1_supported:\n        report.append(\"H1 is **SUPPORTED** - active enforcement improves performance, but H2 evidence is mixed.\\n\")\n    else:\n        report.append(\"The experimental evidence does **NOT** fully support the research hypotheses. \")\n        report.append(\"Further investigation is needed.\\n\")\n    \n    # Save report\n    with open(Path(output_path) / 'experimental_report.md', 'w') as f:\n        f.writelines(report)\n    \n    logger.info(f\"Experimental report saved to {output_path}/experimental_report.md\")\n\ndef main():\n    \"\"\"Main analysis pipeline\"\"\"\n    logging.basicConfig(level=logging.INFO)\n    \n    # Paths\n    results_path = \"/home/runner/work/derp/derp/experiments/exp_20250904_180037/results\"\n    \n    try:\n        # Load and process results\n        df = load_experiment_results(results_path)\n        logger.info(f\"Loaded {len(df)} experimental results\")\n        \n        # Statistical analysis\n        statistical_results = hypothesis_testing(df)\n        \n        # Create visualizations\n        create_visualizations(df, results_path)\n        \n        # Generate report\n        generate_report(df, statistical_results, results_path)\n        \n        # Save processed data\n        df.to_csv(Path(results_path) / 'processed_results.csv', index=False)\n        \n        with open(Path(results_path) / 'statistical_analysis.json', 'w') as f:\n            json.dump(statistical_results, f, indent=2)\n        \n        logger.info(\"Analysis completed successfully!\")\n        \n        return df, statistical_results\n        \n    except Exception as e:\n        logger.error(f\"Analysis failed: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()