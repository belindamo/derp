{"id":"zhang2025","title":"Advancing Deep Learning through Probability Engineering: A Pragmatic Paradigm for Modern AI","authors":"Jianyi Zhang","journal":"arXiv","year":"2025","doi":"10.48550/arXiv.2503.18958","url":"https://arxiv.org/abs/2503.18958","keyAssumptions":"Probability distributions are static objects to be fitted; traditional probabilistic modeling sufficient for modern AI","keyHypotheses":"Actively modifying and reinforcing learned distributions can better address diverse AI requirements than passive fitting","strengths":"Novel paradigm shift; practical applications across multiple domains; systematic expansion of probabilistic modeling role","weaknesses":"High-level conceptual paper; limited technical implementation details; needs empirical validation","citation":"Zhang, J. (2025). Advancing Deep Learning through Probability Engineering: A Pragmatic Paradigm for Modern AI. arXiv:2503.18958","notes":"Directly relevant to DERP - establishes theoretical foundation for active distribution modification","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"ahmadi2024","title":"Distributional Adversarial Loss","authors":"Saba Ahmadi, Siddharth Bhandari, Avrim Blum, Chen Dan, Prabhav Jain","journal":"arXiv","year":"2024","doi":"10.48550/arXiv.2406.03458","url":"https://arxiv.org/abs/2406.03458","keyAssumptions":"Adversarial perturbation sets are point sets; distributional robustness through worst-case analysis","keyHypotheses":"Using distribution families as perturbation sets provides better adversarial robustness guarantees","strengths":"Novel theoretical framework; PAC-learning bounds; unifies randomized smoothing and robust learning","weaknesses":"Limited to theoretical analysis; computational complexity may be high; needs practical algorithms","citation":"Ahmadi, S., et al. (2024). Distributional Adversarial Loss. arXiv:2406.03458","notes":"Shows distributional assumptions can be enforced via adversarial training - relevant to enforcement mechanisms","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"hao2025","title":"Towards Better Generalization via Distributional Input Projection Network","authors":"Yifan Hao, Yanxin Lu, Xinwei Shen, Tong Zhang","journal":"arXiv","year":"2025","doi":"10.48550/arXiv.2506.04690","url":"https://arxiv.org/abs/2506.04690","keyAssumptions":"Standard neural networks process deterministic inputs; smoothness improvements require architectural changes","keyHypotheses":"Projecting inputs to learnable distributions at each layer induces smoother loss landscapes and better generalization","strengths":"Practical implementation; works across architectures (ViTs, LLMs, ResNet); consistent improvements; seamless integration","weaknesses":"Computational overhead; hyperparameter sensitivity; limited theoretical analysis of why it works","citation":"Hao, Y., et al. (2025). Towards Better Generalization via Distributional Input Projection Network. arXiv:2506.04690","notes":"Practical implementation of distribution enforcement throughout network layers - directly applicable to DERP","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"lan2019","title":"A Probabilistic Representation of Deep Learning","authors":"Xinjie Lan, Kenneth E. Barner","journal":"arXiv","year":"2019","doi":"10.48550/arXiv.1908.09772","url":"https://arxiv.org/abs/1908.09772","keyAssumptions":"Deep networks are black boxes; probabilistic interpretation is secondary to performance","keyHypotheses":"DNNs can be explicitly interpreted as Bayesian networks with neurons defining Gibbs distributions","strengths":"Clear theoretical framework; explicit probabilistic interpretation; addresses hierarchy and generalization","weaknesses":"Limited to theoretical analysis; validation only on synthetic data; unclear practical implications","citation":"Lan, X., Barner, K.E. (2019). A Probabilistic Representation of Deep Learning. arXiv:1908.09772","notes":"Foundation for understanding distributional assumptions in neural networks - relevant to theoretical grounding","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"paik2023","title":"Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test","authors":"Seunghoon Paik, Michael Celentano, Alden Green, Ryan J. Tibshirani","journal":"arXiv","year":"2023","doi":"10.48550/arXiv.2309.02422","url":"https://arxiv.org/abs/2309.02422","keyAssumptions":"Classical K-S test limited to 1D; multivariate two-sample testing requires complex methods","keyHypotheses":"Neural networks can implement multivariate generalizations of K-S test via ridge splines","strengths":"Strong theoretical foundation; neural network implementation; asymptotically full power; practical optimization via deep learning","weaknesses":"Computational complexity; requires careful hyperparameter tuning; limited empirical evaluation","citation":"Paik, S., et al. (2023). Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test. arXiv:2309.02422","notes":"Direct implementation of statistical testing via neural networks - key to our Random Probe methodology","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"dang2024","title":"Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse in Conditional and Hierarchical VAEs","authors":"Hien Dang, Tho Tran, Tan Nguyen, Nhat Ho","journal":"ICLR","year":"2024","doi":"","url":"https://arxiv.org/abs/2306.05023","keyAssumptions":"Posterior collapse mainly affects standard VAEs; conditional and hierarchical variants less studied","keyHypotheses":"Correlation in conditional VAE and learnable encoder variance in hierarchical VAE cause posterior collapse","strengths":"Rigorous theoretical analysis; extends beyond standard VAE; empirical validation on linear and non-linear cases","weaknesses":"Limited to specific VAE variants; theoretical analysis mostly for linear cases; limited practical solutions","citation":"Dang, H., et al. (2024). Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse in Conditional and Hierarchical VAEs. ICLR","notes":"Theoretical analysis of distributional assumption failures in VAE - critical for understanding when enforcement fails","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"lucas2019","title":"Don't Blame the ELBO! A Linear VAE Perspective on Posterior Collapse","authors":"James Lucas, George Tucker, Roger B. Grosse, Mohammad Norouzi","journal":"NeurIPS","year":"2019","doi":"","url":"https://proceedings.neurips.cc/paper/2019/file/7e3315fe390974fcf25e44a9445bd821-Paper.pdf","keyAssumptions":"ELBO objective causes posterior collapse through KL divergence term; optimization issues secondary","keyHypotheses":"Posterior collapse arises from local maxima in loss surface, not ELBO formulation itself","strengths":"Clear theoretical analysis via linear VAE; connection to pPCA; challenges conventional wisdom; tractable analysis","weaknesses":"Limited to linear case; gap between linear analysis and deep VAE behavior; limited practical solutions","citation":"Lucas, J., et al. (2019). Don't Blame the ELBO! A Linear VAE Perspective on Posterior Collapse. NeurIPS","notes":"Fundamental insight that posterior collapse is optimization problem, not objective design - informs when distribution enforcement can succeed","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"wang2023","title":"Posterior Collapse and Latent Variable Non-identifiability","authors":"Yixin Wang, David M. Blei, John P. Cunningham","journal":"arXiv","year":"2023","doi":"","url":"https://arxiv.org/abs/2301.00537","keyAssumptions":"Posterior collapse is VAE-specific phenomenon; neural networks or variational approximation cause the issue","keyHypotheses":"Posterior collapse occurs if and only if latent variables are non-identifiable in the generative model","strengths":"Fundamental theoretical insight; connects to identifiability theory; proposes practical solution with bijective maps","weaknesses":"Complex mathematical formulation; computational overhead of bijective maps; limited empirical evaluation","citation":"Wang, Y., Blei, D.M., Cunningham, J.P. (2023). Posterior Collapse and Latent Variable Non-identifiability. arXiv:2301.00537","notes":"Critical theoretical foundation - shows distributional assumptions must be identifiable for enforcement to work","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"takida2022","title":"Preventing Posterior Collapse Induced by Oversmoothing in Gaussian VAE","authors":"Yuhta Takida, Wei-Hsiang Liao, Toshimitsu Uesaka, Shusuke Takahashi, Yuki Mitsufuji","journal":"arXiv","year":"2022","doi":"","url":"https://arxiv.org/abs/2102.08663","keyAssumptions":"Fixed variance parameters in Gaussian VAE; oversmoothing is unavoidable side effect","keyHypotheses":"Adaptive variance parameter control can prevent oversmoothing-induced posterior collapse","strengths":"Clear connection between variance and posterior collapse; practical AR-ELBO algorithm; improved FID scores","weaknesses":"Limited to Gaussian VAE; hyperparameter sensitivity; computational overhead of adaptive methods","citation":"Takida, Y., et al. (2022). Preventing Posterior Collapse Induced by Oversmoothing in Gaussian VAE. arXiv:2102.08663","notes":"Shows importance of proper variance parameter tuning - relevant to parameter selection in distribution enforcement","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"lin2019","title":"Balancing Reconstruction Quality and Regularisation in Evidence Lower Bound for Variational Autoencoders","authors":"Shuyu Lin, Stephen Roberts, Niki Trigoni, Ronald Clark","journal":"arXiv","year":"2019","doi":"","url":"https://arxiv.org/abs/1909.03765","keyAssumptions":"Fixed noise variance in Gaussian likelihood; manual tuning of reconstruction-regularization trade-off","keyHypotheses":"Learning noise variance in p(x|z) automatically provides optimal reconstruction-regularization balance","strengths":"Simple and intuitive solution; automatic trade-off optimization; uncertainty estimation capability; improved generation quality","weaknesses":"Limited to Gaussian likelihoods; may not generalize to other distributions; uncertainty estimation not thoroughly evaluated","citation":"Lin, S., et al. (2019). Balancing Reconstruction Quality and Regularisation in Evidence Lower Bound for Variational Autoencoders. arXiv:1909.03765","notes":"Shows how distributional parameters can be learned for better enforcement - relevant to automatic parameter selection","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"seo2024","title":"Rate-Adaptive Quantization: A Multi-Rate Codebook Adaptation for Vector Quantization-based Generative Models","authors":"Jiwan Seo, Joonhyuk Kang","journal":"arXiv","year":"2024","doi":"","url":"https://arxiv.org/abs/2405.14222","keyAssumptions":"Single fixed-rate codebooks in VQ models; extensive retraining needed for different rates","keyHypotheses":"Multi-rate codebook adaptation from single baseline can provide flexible rate-distortion trade-offs","strengths":"Single system handles diverse requirements; data-driven approach; practical clustering procedure; outperforms fixed-rate baselines","weaknesses":"Complexity in codebook management; potential overfitting to training distribution; computational overhead","citation":"Seo, J., Kang, J. (2024). Rate-Adaptive Quantization: A Multi-Rate Codebook Adaptation for Vector Quantization-based Generative Models. arXiv:2405.14222","notes":"Shows adaptive codebook learning - relevant to making distribution enforcement adaptive to data requirements","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"huijben2024","title":"Residual Quantization with Implicit Neural Codebooks","authors":"Iris A. M. Huijben, Matthijs Douze, Matthew Muckley, Ruud J. G. van Sloun, Jakob Verbeek","journal":"ICML","year":"2024","doi":"","url":"https://arxiv.org/abs/2401.14732","keyAssumptions":"Fixed codebooks per quantization step; error distribution independence across steps","keyHypotheses":"Specialized codebooks dependent on previous quantization steps can account for error distribution dependencies","strengths":"Strong theoretical motivation; superior performance (12 vs 16 bytes); accounts for distribution dependencies; practical implementation","weaknesses":"Increased model complexity; computational overhead; limited to residual quantization; needs careful initialization","citation":"Huijben, I.A.M., et al. (2024). Residual Quantization with Implicit Neural Codebooks. ICML","notes":"Shows how accounting for distributional dependencies improves quantization - relevant to multi-step distribution enforcement","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"zheng2023","title":"Online Clustered Codebook","authors":"Chuanxia Zheng, Andrea Vedaldi","journal":"arXiv","year":"2023","doi":"10.48550/arXiv.2307.15139","url":"https://arxiv.org/abs/2307.15139","keyAssumptions":"Equal update probability for all codevectors; gradient-based updates sufficient for all codevectors","keyHypotheses":"Clustered updates using encoded features as anchors can revive dead codevectors and improve coverage","strengths":"Addresses fundamental codebook collapse problem; simple integration; improved codebook utilization; general applicability","weaknesses":"Heuristic approach; limited theoretical analysis; computational overhead; may introduce bias toward certain features","citation":"Zheng, C., Vedaldi, A. (2023). Online Clustered Codebook. arXiv:2307.15139","notes":"Addresses distributional coverage in codebook learning - relevant to ensuring comprehensive distribution enforcement","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"tamkin2023","title":"Codebook Features: Sparse and Discrete Interpretability for Neural Networks","authors":"Alex Tamkin, Mohammad Taufeeque","journal":"Stanford AI Lab Blog","year":"2023","doi":"","url":"https://ai.stanford.edu/blog/codebook-features/","keyAssumptions":"Dense continuous representations necessary for neural network expressivity; interpretability requires sacrificing performance","keyHypotheses":"Sparse discrete representations via VQ can maintain performance while improving interpretability","strengths":"Practical implementation; minimal accuracy drop; interpretable representations; works with transformers","weaknesses":"Limited theoretical analysis; interpretability gains not quantified; computational overhead not analyzed","citation":"Tamkin, A., Taufeeque, M. (2023). Codebook Features: Sparse and Discrete Interpretability for Neural Networks. Stanford AI Lab Blog","notes":"Shows VQ can maintain distributional properties while improving interpretability - relevant to verifiable distribution enforcement","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"bosman2023","title":"Robustness Distributions in Neural Network Verification","authors":"Annelot W. Bosman, Aaron Berger, Holger H. Hoos, Jan N. van Rijn","journal":"JAIR","year":"2023","doi":"","url":"https://www.jair.org/index.php/jair/article/view/18403","keyAssumptions":"Binary robust/non-robust classification sufficient; individual input analysis adequate","keyHypotheses":"Critical epsilon distributions follow log-normal distribution; K-S tests can verify distributional properties of robustness","strengths":"Rigorous statistical analysis; K-S test validation; extensive empirical evaluation; novel robustness distribution perspective","weaknesses":"Limited to MNIST; computational cost of complete verification; theoretical analysis incomplete","citation":"Bosman, A.W., et al. (2023). Robustness Distributions in Neural Network Verification. JAIR","notes":"Direct application of K-S testing to neural network analysis - validates our Random Probe approach for verification","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"song2024","title":"Toward Architecture-Agnostic Local Control of Posterior Collapse in VAEs","authors":"Hyunsoo Song, Seungwhan Kim, Seungkyu Lee","journal":"arXiv","year":"2024","doi":"10.48550/arXiv.2508.12530","url":"https://arxiv.org/abs/2508.12530","keyAssumptions":"Structural constraints required for latent identifiability; global posterior collapse measures sufficient","keyHypotheses":"Local posterior collapse definition relaxes network constraints; Latent Reconstruction loss controls collapse without architectural restrictions","strengths":"Architecture-agnostic approach; mathematical properties of injective functions; tested on diverse datasets","weaknesses":"Limited theoretical analysis of LR loss; computational overhead not analyzed","citation":"Song, H., et al. (2024). Toward Architecture-Agnostic Local Control of Posterior Collapse in VAEs. arXiv:2508.12530","notes":"Introduces local posterior collapse and architecture-independent solutions - relevant to flexible distribution enforcement","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"novitskiy2025","title":"VIVAT: Virtuous Improving VAE Training through Artifact Mitigation","authors":"Lev Novitskiy, Viacheslav Vasilev, Maria Kovaleva, Vladimir Arkhipkin, Denis Dimitrov","journal":"arXiv","year":"2025","doi":"10.48550/arXiv.2506.07863","url":"https://arxiv.org/abs/2506.07863","keyAssumptions":"Training artifacts are unavoidable; radical architectural changes needed for improvements","keyHypotheses":"Systematic artifact mitigation through straightforward modifications improves VAE performance","strengths":"Practical approach; state-of-the-art reconstruction metrics; preserves KL-VAE simplicity; actionable insights","weaknesses":"Limited to KL-VAE framework; artifact classification may not be exhaustive","citation":"Novitskiy, L., et al. (2025). VIVAT: Virtuous Improving VAE Training through Artifact Mitigation. arXiv:2506.07863","notes":"Addresses practical VAE training issues - complements distribution enforcement with artifact mitigation","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"tsong2024","title":"Scale-VAE: Preventing Posterior Collapse in Variational Autoencoder","authors":"Tianbao Song, Jingbo Sun, Xin Liu, Weiming Peng","journal":"LREC","year":"2024","doi":"","url":"https://aclanthology.org/2024.lrec-main.1250.pdf","keyAssumptions":"KL term must be larger than positive constant; posterior distributions are fixed during training","keyHypotheses":"Scaling posterior mean dimensions while preserving relative relationships prevents collapse","strengths":"Simple method; maintains relative posterior relationships; similar training cost to basic VAE","weaknesses":"Limited theoretical analysis; scaling factor selection not well justified","citation":"Song, T., et al. (2024). Scale-VAE: Preventing Posterior Collapse in Variational Autoencoder. LREC","notes":"Novel scaling approach to posterior collapse - relevant to distribution parameter manipulation","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"razavi2019","title":"Preventing Posterior Collapse with δ-VAEs","authors":"Ali Razavi, Aaron van den Oord, Ben Poole, Oriol Vinyals","journal":"ICLR","year":"2019","doi":"","url":"https://arxiv.org/abs/1901.03416","keyAssumptions":"Powerful decoders cause latent variable collapse; variational family must be constrained","keyHypotheses":"Constraining posterior to have minimum distance from prior prevents collapse while preserving information","strengths":"State-of-the-art log-likelihood on CIFAR-10 and ImageNet; theoretical foundation; combines VAE and autoregressive models","weaknesses":"Computational overhead of distance constraints; hyperparameter sensitivity","citation":"Razavi, A., et al. (2019). Preventing Posterior Collapse with δ-VAEs. ICLR","notes":"Seminal work on constraining posterior distributions - direct precedent for distribution enforcement","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"petit2021","title":"Preventing Posterior Collapse in Variational Autoencoders for Text Generation via Decoder Regularization","authors":"Alban Petit, Caio Corro","journal":"arXiv","year":"2021","doi":"10.48550/arXiv.2110.14945","url":"https://arxiv.org/abs/2110.14945","keyAssumptions":"Minimizing reconstruction error alone is sufficient; decoder regularization not necessary","keyHypotheses":"Fraternal dropout regularization in decoder prevents posterior collapse","strengths":"Novel regularization approach; evaluated on text generation; targets decoder directly","weaknesses":"Limited to text domain; computational overhead not quantified","citation":"Petit, A., Corro, C. (2021). Preventing Posterior Collapse in VAEs for Text Generation via Decoder Regularization. arXiv:2110.14945","notes":"Decoder-focused approach to posterior collapse prevention - complements encoder-side distribution enforcement","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"melis2022","title":"Mutual Information Constraints for Monte-Carlo Objectives to Prevent Posterior Collapse","authors":"Gábor Melis, András György, Phil Blunsom","journal":"JMLR","year":"2022","doi":"","url":"https://arxiv.org/abs/2012.00708","keyAssumptions":"Single-sample Monte-Carlo objectives sufficient; KL divergence estimation between q(z|x) and prior adequate","keyHypotheses":"Multi-sample Monte-Carlo objectives with mutual information constraints prevent posterior collapse","strengths":"Theoretical foundation; addresses underestimation of latent utility; multi-sample objectives; improved rate-distortion","weaknesses":"Computational complexity; limited to language modeling evaluation","citation":"Melis, G., et al. (2022). Mutual Information Constraints for Monte-Carlo Objectives to Prevent Posterior Collapse. JMLR","notes":"Addresses theoretical underpinnings of posterior collapse - provides MI framework for distribution enforcement","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"tu2021","title":"Neighbor Embedding Variational Autoencoder","authors":"Renfei Tu, Yang Liu, Yongzeng Xue, Cheng Wang, Maozu Guo","journal":"ICML","year":"2021","doi":"","url":"https://arxiv.org/abs/2103.11349","keyAssumptions":"VAE loss only depends on p(x|θ) not p(x,z|θ); latent organization secondary to reconstruction","keyHypotheses":"Explicit constraints ensuring inputs close in input space are close in latent space prevents collapse","strengths":"Qualitatively different latent representations; majority of dimensions remain active; easily pluggable","weaknesses":"Limited theoretical understanding; computational overhead not analyzed","citation":"Tu, R., et al. (2021). Neighbor Embedding Variational Autoencoder. ICML","notes":"Introduces locality constraints for latent space organization - relevant to spatial distribution enforcement","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"lygerakis2023","title":"CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse","authors":"Fotios Lygerakis, Elmar Rueckert","journal":"arXiv","year":"2023","doi":"10.48550/arXiv.2309.02968","url":"https://arxiv.org/abs/2309.02968","keyAssumptions":"VAE objective function limitations cause posterior collapse; latent representations become input-independent","keyHypotheses":"Contrastive objective maximizing mutual information between similar visual inputs prevents collapse","strengths":"Outperforms state-of-the-art collapse prevention; maximizes information flow; augments original VAE","weaknesses":"Limited to visual data; contrastive learning overhead; hyperparameter sensitivity","citation":"Lygerakis, F., Rueckert, E. (2023). CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse. arXiv:2309.02968","notes":"Uses contrastive learning for distribution enforcement - alternative approach to statistical constraints","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"fang2025","title":"Enhancing Vector Quantization with Distributional Matching: A Theoretical and Empirical Study","authors":"Xianghong Fang, Litao Guo, Hengchao Chen, et al.","journal":"arXiv","year":"2025","doi":"10.48550/arXiv.2506.15078","url":"https://arxiv.org/abs/2506.15078","keyAssumptions":"Training instability from gradient discrepancy; codebook collapse from underutilization acceptable","keyHypotheses":"Wasserstein distance alignment between feature and code vector distributions prevents collapse","strengths":"Near 100% codebook utilization; significantly reduced quantization error; theoretical and empirical validation","weaknesses":"Computational overhead of Wasserstein distance; limited to VQ applications","citation":"Fang, X., et al. (2025). Enhancing Vector Quantization with Distributional Matching. arXiv:2506.15078","notes":"Direct application of distribution matching to VQ - validates distributional enforcement for codebook learning","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"wu2018","title":"Learning Product Codebooks Using Vector-Quantized Autoencoders for Image Retrieval","authors":"Hanwei Wu, Markus Flierl","journal":"arXiv","year":"2018","doi":"10.48550/arXiv.1807.04629","url":"https://arxiv.org/abs/1807.04629","keyAssumptions":"Standard VQ-VAE regularization adequate; fixed regularization strength sufficient","keyHypotheses":"Hyperparameter balancing vector quantizer and reconstruction enables regularization control","strengths":"Information-theoretic framework; hyperparameter search range provided; end-to-end learning","weaknesses":"Limited to image retrieval; hyperparameter sensitivity not fully explored","citation":"Wu, H., Flierl, M. (2018). Learning Product Codebooks Using Vector-Quantized Autoencoders for Image Retrieval. arXiv:1807.04629","notes":"Early work on VQ-VAE regularization control - precursor to active distribution enforcement","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"takida2022b","title":"SQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization","authors":"Yuhta Takida, Takashi Shibuya, Wei-Hsiang Liao, et al.","journal":"ICML","year":"2022","doi":"","url":"https://arxiv.org/abs/2205.07547","keyAssumptions":"VQ-VAE heuristics necessary; deterministic quantization required","keyHypotheses":"Self-annealing stochastic quantization improves codebook utilization without heuristics","strengths":"Improved codebook utilization; follows standard VAE framework; superior performance in vision and speech","weaknesses":"Complexity of stochastic quantization; theoretical analysis incomplete","citation":"Takida, Y., et al. (2022). SQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization. ICML","notes":"Stochastic approach to VQ training - alternative to deterministic distribution enforcement","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"khalil2023","title":"LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations","authors":"Ahmed Khalil, Robert Piechocki, Raul Santos-Rodriguez","journal":"arXiv","year":"2023","doi":"10.48550/arXiv.2310.09382","url":"https://arxiv.org/abs/2310.09382","keyAssumptions":"Standard VQ codebook structure adequate; unstructured quantization sufficient","keyHypotheses":"Learnable lattice structure over discrete embeddings prevents codebook collapse","strengths":"Lower reconstruction errors; faster training; constant parameter count; acts as deterrent against collapse","weaknesses":"Limited to lattice structures; theoretical analysis of lattice benefits incomplete","citation":"Khalil, A., et al. (2023). LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations. arXiv:2310.09382","notes":"Structured approach to VQ using lattices - provides geometric foundation for distribution enforcement","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"simic2020","title":"Testing for Normality with Neural Networks","authors":"Miloš Simić","journal":"arXiv","year":"2020","doi":"10.48550/arXiv.2009.13831","url":"https://arxiv.org/abs/2009.13831","keyAssumptions":"Traditional statistical tests (Shapiro-Wilk, Anderson-Darling) are optimal for normality testing","keyHypotheses":"Neural networks can outperform traditional normality tests by treating the problem as binary classification","strengths":"AUROC ≈ 1 (perfect binary classifier); >96% accuracy on large samples; outperforms traditional tests","weaknesses":"Limited to normality testing; broader applicability to other distributions unclear","citation":"Simić, M. (2020). Testing for Normality with Neural Networks. arXiv:2009.13831","notes":"Demonstrates neural networks can effectively perform distributional verification - validates NN-based statistical testing","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"kim2023","title":"A Neural Network-Based Approach to Normality Testing for Dependent Data","authors":"Minwoo Kim, Marc G Genton, Raphael Huser, Stefano Castruccio","journal":"arXiv","year":"2023","doi":"10.48550/arXiv.2310.10422","url":"https://arxiv.org/abs/2310.10422","keyAssumptions":"Independent and identically distributed data assumptions sufficient; spatial/temporal dependence negligible","keyHypotheses":"Neural networks can non-linearly incorporate existing statistics to assess normality for dependent data","strengths":"Handles dependent data; superior statistical power; incorporates multiple statistics; real-world validation","weaknesses":"Limited to normality testing; complexity of neural network approach","citation":"Kim, M., et al. (2023). A Neural Network-Based Approach to Normality Testing for Dependent Data. arXiv:2310.10422","notes":"Extends neural network statistical testing to dependent data - relevant for sequential/spatial distribution enforcement","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"fraiman2022","title":"A Cramér-Wold Theorem for Elliptical Distributions","authors":"Ricardo Fraiman, Leonardo Moreno, Thomas Ransford","journal":"arXiv","year":"2022","doi":"10.48550/arXiv.2206.13612","url":"https://arxiv.org/abs/2206.13612","keyAssumptions":"All one-dimensional projections needed for multivariate distribution characterization","keyHypotheses":"For elliptical distributions, (d²+d)/2 specific line projections suffice for complete characterization","strengths":"Optimal number of projections; no moment finiteness assumptions; statistical test derivation; practical applications","weaknesses":"Limited to elliptical distributions; computational complexity for high dimensions","citation":"Fraiman, R., et al. (2022). A Cramér-Wold Theorem for Elliptical Distributions. arXiv:2206.13612","notes":"Provides optimal projection bounds for elliptical distributions - critical for efficient random probe implementation","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"fraiman2021","title":"Application of the Cramér-Wold Theorem to Testing for Invariance under Group Actions","authors":"Ricardo Fraiman, Leonardo Moreno, Thomas Ransford","journal":"arXiv","year":"2021","doi":"10.48550/arXiv.2109.01041","url":"https://arxiv.org/abs/2109.01041","keyAssumptions":"High-dimensional distributional testing requires complex multivariate methods","keyHypotheses":"One-dimensional projections via Cramér-Wold theorem enable powerful, dimension-independent invariance testing","strengths":"Powerful and computationally efficient; dimension-independent; extends to infinite-dimensional spaces","weaknesses":"Limited to invariance testing; requires group structure assumptions","citation":"Fraiman, R., et al. (2021). Application of the Cramér-Wold Theorem to Testing for Invariance under Group Actions. arXiv:2109.01041","notes":"Demonstrates practical application of random projections for statistical testing - direct precedent for Random Probe methodology","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"chen2024","title":"Model Checking for High Dimensional Generalized Linear Models Based on Random Projections","authors":"Wen Chen, Jie Liu, Heng Peng, Falong Tan, Lixing Zhu","journal":"arXiv","year":"2024","doi":"10.48550/arXiv.2412.10721","url":"https://arxiv.org/abs/2412.10721","keyAssumptions":"Existing goodness-of-fit tests fail in high dimensions; normality of parameter estimators required","keyHypotheses":"Random projections enable goodness-of-fit testing when dimension substantially exceeds sample size","strengths":"Works in high dimensions; only requires convergence rate of estimators; alleviates curse of dimensionality","weaknesses":"Limited to GLM setting; bandwidth parameter selection critical","citation":"Chen, W., et al. (2024). Model Checking for High Dimensional GLMs Based on Random Projections. arXiv:2412.10721","notes":"Recent work on random projections for high-dimensional model checking - validates our Random Probe approach","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"ayyala2020","title":"Covariance Matrix Testing in High Dimension Using Random Projections","authors":"Deepak Nag Ayyala, Santu Ghosh, Daniel F. Linder","journal":"arXiv","year":"2020","doi":"10.48550/arXiv.2011.08282","url":"https://arxiv.org/abs/2011.08282","keyAssumptions":"Traditional multivariate asymptotic theory valid; dimension smaller than sample size","keyHypotheses":"Random projections alleviate curse of dimensionality for covariance matrix testing","strengths":"Handles high dimensions; computationally efficient; strong control of type I error; real data validation","weaknesses":"Limited to covariance testing; projection dimension selection not well explored","citation":"Ayyala, D.N., et al. (2020). Covariance Matrix Testing in High Dimension Using Random Projections. arXiv:2011.08282","notes":"CRAMP method demonstrates random projections for statistical testing - validates Random Probe framework","addedDate":"2025-08-31T01:30:00.000Z"}
{"id":"bobkov2025","title":"A Quantitative Cramér-Wold Theorem for Zolotarev Distances","authors":"Sergey G. Bobkov, Friedrich Götze","journal":"arXiv","year":"2025","doi":"10.48550/arXiv.2506.17745","url":"https://arxiv.org/abs/2506.17745","keyAssumptions":"Classical Cramér-Wold theorem provides qualitative characterization only","keyHypotheses":"Quantitative bounds for Zolotarev distances between measures via one-dimensional projections","strengths":"Quantitative bounds; extends classical theory; applies to general probability measures","weaknesses":"Theoretical focus; practical implications not fully explored; limited empirical validation","citation":"Bobkov, S.G., Götze, F. (2025). A Quantitative Cramér-Wold Theorem for Zolotarev Distances. arXiv:2506.17745","notes":"Recent theoretical advance in Cramér-Wold theory - provides quantitative foundation for Random Probe bounds","addedDate":"2025-08-31T01:30:00.000Z"}