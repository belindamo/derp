{"id":"zhang2025","title":"Advancing Deep Learning through Probability Engineering: A Pragmatic Paradigm for Modern AI","authors":"Jianyi Zhang","journal":"arXiv","year":"2025","doi":"10.48550/arXiv.2503.18958","url":"https://arxiv.org/abs/2503.18958","keyAssumptions":"Probability distributions are static objects to be fitted; traditional probabilistic modeling sufficient for modern AI","keyHypotheses":"Actively modifying and reinforcing learned distributions can better address diverse AI requirements than passive fitting","strengths":"Novel paradigm shift; practical applications across multiple domains; systematic expansion of probabilistic modeling role","weaknesses":"High-level conceptual paper; limited technical implementation details; needs empirical validation","citation":"Zhang, J. (2025). Advancing Deep Learning through Probability Engineering: A Pragmatic Paradigm for Modern AI. arXiv:2503.18958","notes":"Directly relevant to DERP - establishes theoretical foundation for active distribution modification","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"ahmadi2024","title":"Distributional Adversarial Loss","authors":"Saba Ahmadi, Siddharth Bhandari, Avrim Blum, Chen Dan, Prabhav Jain","journal":"arXiv","year":"2024","doi":"10.48550/arXiv.2406.03458","url":"https://arxiv.org/abs/2406.03458","keyAssumptions":"Adversarial perturbation sets are point sets; distributional robustness through worst-case analysis","keyHypotheses":"Using distribution families as perturbation sets provides better adversarial robustness guarantees","strengths":"Novel theoretical framework; PAC-learning bounds; unifies randomized smoothing and robust learning","weaknesses":"Limited to theoretical analysis; computational complexity may be high; needs practical algorithms","citation":"Ahmadi, S., et al. (2024). Distributional Adversarial Loss. arXiv:2406.03458","notes":"Shows distributional assumptions can be enforced via adversarial training - relevant to enforcement mechanisms","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"hao2025","title":"Towards Better Generalization via Distributional Input Projection Network","authors":"Yifan Hao, Yanxin Lu, Xinwei Shen, Tong Zhang","journal":"arXiv","year":"2025","doi":"10.48550/arXiv.2506.04690","url":"https://arxiv.org/abs/2506.04690","keyAssumptions":"Standard neural networks process deterministic inputs; smoothness improvements require architectural changes","keyHypotheses":"Projecting inputs to learnable distributions at each layer induces smoother loss landscapes and better generalization","strengths":"Practical implementation; works across architectures (ViTs, LLMs, ResNet); consistent improvements; seamless integration","weaknesses":"Computational overhead; hyperparameter sensitivity; limited theoretical analysis of why it works","citation":"Hao, Y., et al. (2025). Towards Better Generalization via Distributional Input Projection Network. arXiv:2506.04690","notes":"Practical implementation of distribution enforcement throughout network layers - directly applicable to DERP","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"lan2019","title":"A Probabilistic Representation of Deep Learning","authors":"Xinjie Lan, Kenneth E. Barner","journal":"arXiv","year":"2019","doi":"10.48550/arXiv.1908.09772","url":"https://arxiv.org/abs/1908.09772","keyAssumptions":"Deep networks are black boxes; probabilistic interpretation is secondary to performance","keyHypotheses":"DNNs can be explicitly interpreted as Bayesian networks with neurons defining Gibbs distributions","strengths":"Clear theoretical framework; explicit probabilistic interpretation; addresses hierarchy and generalization","weaknesses":"Limited to theoretical analysis; validation only on synthetic data; unclear practical implications","citation":"Lan, X., Barner, K.E. (2019). A Probabilistic Representation of Deep Learning. arXiv:1908.09772","notes":"Foundation for understanding distributional assumptions in neural networks - relevant to theoretical grounding","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"paik2023","title":"Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test","authors":"Seunghoon Paik, Michael Celentano, Alden Green, Ryan J. Tibshirani","journal":"arXiv","year":"2023","doi":"10.48550/arXiv.2309.02422","url":"https://arxiv.org/abs/2309.02422","keyAssumptions":"Classical K-S test limited to 1D; multivariate two-sample testing requires complex methods","keyHypotheses":"Neural networks can implement multivariate generalizations of K-S test via ridge splines","strengths":"Strong theoretical foundation; neural network implementation; asymptotically full power; practical optimization via deep learning","weaknesses":"Computational complexity; requires careful hyperparameter tuning; limited empirical evaluation","citation":"Paik, S., et al. (2023). Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test. arXiv:2309.02422","notes":"Direct implementation of statistical testing via neural networks - key to our Random Probe methodology","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"dang2024","title":"Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse in Conditional and Hierarchical VAEs","authors":"Hien Dang, Tho Tran, Tan Nguyen, Nhat Ho","journal":"ICLR","year":"2024","doi":"","url":"https://arxiv.org/abs/2306.05023","keyAssumptions":"Posterior collapse mainly affects standard VAEs; conditional and hierarchical variants less studied","keyHypotheses":"Correlation in conditional VAE and learnable encoder variance in hierarchical VAE cause posterior collapse","strengths":"Rigorous theoretical analysis; extends beyond standard VAE; empirical validation on linear and non-linear cases","weaknesses":"Limited to specific VAE variants; theoretical analysis mostly for linear cases; limited practical solutions","citation":"Dang, H., et al. (2024). Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse in Conditional and Hierarchical VAEs. ICLR","notes":"Theoretical analysis of distributional assumption failures in VAE - critical for understanding when enforcement fails","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"lucas2019","title":"Don't Blame the ELBO! A Linear VAE Perspective on Posterior Collapse","authors":"James Lucas, George Tucker, Roger B. Grosse, Mohammad Norouzi","journal":"NeurIPS","year":"2019","doi":"","url":"https://proceedings.neurips.cc/paper/2019/file/7e3315fe390974fcf25e44a9445bd821-Paper.pdf","keyAssumptions":"ELBO objective causes posterior collapse through KL divergence term; optimization issues secondary","keyHypotheses":"Posterior collapse arises from local maxima in loss surface, not ELBO formulation itself","strengths":"Clear theoretical analysis via linear VAE; connection to pPCA; challenges conventional wisdom; tractable analysis","weaknesses":"Limited to linear case; gap between linear analysis and deep VAE behavior; limited practical solutions","citation":"Lucas, J., et al. (2019). Don't Blame the ELBO! A Linear VAE Perspective on Posterior Collapse. NeurIPS","notes":"Fundamental insight that posterior collapse is optimization problem, not objective design - informs when distribution enforcement can succeed","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"wang2023","title":"Posterior Collapse and Latent Variable Non-identifiability","authors":"Yixin Wang, David M. Blei, John P. Cunningham","journal":"arXiv","year":"2023","doi":"","url":"https://arxiv.org/abs/2301.00537","keyAssumptions":"Posterior collapse is VAE-specific phenomenon; neural networks or variational approximation cause the issue","keyHypotheses":"Posterior collapse occurs if and only if latent variables are non-identifiable in the generative model","strengths":"Fundamental theoretical insight; connects to identifiability theory; proposes practical solution with bijective maps","weaknesses":"Complex mathematical formulation; computational overhead of bijective maps; limited empirical evaluation","citation":"Wang, Y., Blei, D.M., Cunningham, J.P. (2023). Posterior Collapse and Latent Variable Non-identifiability. arXiv:2301.00537","notes":"Critical theoretical foundation - shows distributional assumptions must be identifiable for enforcement to work","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"takida2022","title":"Preventing Posterior Collapse Induced by Oversmoothing in Gaussian VAE","authors":"Yuhta Takida, Wei-Hsiang Liao, Toshimitsu Uesaka, Shusuke Takahashi, Yuki Mitsufuji","journal":"arXiv","year":"2022","doi":"","url":"https://arxiv.org/abs/2102.08663","keyAssumptions":"Fixed variance parameters in Gaussian VAE; oversmoothing is unavoidable side effect","keyHypotheses":"Adaptive variance parameter control can prevent oversmoothing-induced posterior collapse","strengths":"Clear connection between variance and posterior collapse; practical AR-ELBO algorithm; improved FID scores","weaknesses":"Limited to Gaussian VAE; hyperparameter sensitivity; computational overhead of adaptive methods","citation":"Takida, Y., et al. (2022). Preventing Posterior Collapse Induced by Oversmoothing in Gaussian VAE. arXiv:2102.08663","notes":"Shows importance of proper variance parameter tuning - relevant to parameter selection in distribution enforcement","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"lin2019","title":"Balancing Reconstruction Quality and Regularisation in Evidence Lower Bound for Variational Autoencoders","authors":"Shuyu Lin, Stephen Roberts, Niki Trigoni, Ronald Clark","journal":"arXiv","year":"2019","doi":"","url":"https://arxiv.org/abs/1909.03765","keyAssumptions":"Fixed noise variance in Gaussian likelihood; manual tuning of reconstruction-regularization trade-off","keyHypotheses":"Learning noise variance in p(x|z) automatically provides optimal reconstruction-regularization balance","strengths":"Simple and intuitive solution; automatic trade-off optimization; uncertainty estimation capability; improved generation quality","weaknesses":"Limited to Gaussian likelihoods; may not generalize to other distributions; uncertainty estimation not thoroughly evaluated","citation":"Lin, S., et al. (2019). Balancing Reconstruction Quality and Regularisation in Evidence Lower Bound for Variational Autoencoders. arXiv:1909.03765","notes":"Shows how distributional parameters can be learned for better enforcement - relevant to automatic parameter selection","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"seo2024","title":"Rate-Adaptive Quantization: A Multi-Rate Codebook Adaptation for Vector Quantization-based Generative Models","authors":"Jiwan Seo, Joonhyuk Kang","journal":"arXiv","year":"2024","doi":"","url":"https://arxiv.org/abs/2405.14222","keyAssumptions":"Single fixed-rate codebooks in VQ models; extensive retraining needed for different rates","keyHypotheses":"Multi-rate codebook adaptation from single baseline can provide flexible rate-distortion trade-offs","strengths":"Single system handles diverse requirements; data-driven approach; practical clustering procedure; outperforms fixed-rate baselines","weaknesses":"Complexity in codebook management; potential overfitting to training distribution; computational overhead","citation":"Seo, J., Kang, J. (2024). Rate-Adaptive Quantization: A Multi-Rate Codebook Adaptation for Vector Quantization-based Generative Models. arXiv:2405.14222","notes":"Shows adaptive codebook learning - relevant to making distribution enforcement adaptive to data requirements","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"huijben2024","title":"Residual Quantization with Implicit Neural Codebooks","authors":"Iris A. M. Huijben, Matthijs Douze, Matthew Muckley, Ruud J. G. van Sloun, Jakob Verbeek","journal":"ICML","year":"2024","doi":"","url":"https://arxiv.org/abs/2401.14732","keyAssumptions":"Fixed codebooks per quantization step; error distribution independence across steps","keyHypotheses":"Specialized codebooks dependent on previous quantization steps can account for error distribution dependencies","strengths":"Strong theoretical motivation; superior performance (12 vs 16 bytes); accounts for distribution dependencies; practical implementation","weaknesses":"Increased model complexity; computational overhead; limited to residual quantization; needs careful initialization","citation":"Huijben, I.A.M., et al. (2024). Residual Quantization with Implicit Neural Codebooks. ICML","notes":"Shows how accounting for distributional dependencies improves quantization - relevant to multi-step distribution enforcement","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"zheng2023","title":"Online Clustered Codebook","authors":"Chuanxia Zheng, Andrea Vedaldi","journal":"arXiv","year":"2023","doi":"10.48550/arXiv.2307.15139","url":"https://arxiv.org/abs/2307.15139","keyAssumptions":"Equal update probability for all codevectors; gradient-based updates sufficient for all codevectors","keyHypotheses":"Clustered updates using encoded features as anchors can revive dead codevectors and improve coverage","strengths":"Addresses fundamental codebook collapse problem; simple integration; improved codebook utilization; general applicability","weaknesses":"Heuristic approach; limited theoretical analysis; computational overhead; may introduce bias toward certain features","citation":"Zheng, C., Vedaldi, A. (2023). Online Clustered Codebook. arXiv:2307.15139","notes":"Addresses distributional coverage in codebook learning - relevant to ensuring comprehensive distribution enforcement","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"tamkin2023","title":"Codebook Features: Sparse and Discrete Interpretability for Neural Networks","authors":"Alex Tamkin, Mohammad Taufeeque","journal":"Stanford AI Lab Blog","year":"2023","doi":"","url":"https://ai.stanford.edu/blog/codebook-features/","keyAssumptions":"Dense continuous representations necessary for neural network expressivity; interpretability requires sacrificing performance","keyHypotheses":"Sparse discrete representations via VQ can maintain performance while improving interpretability","strengths":"Practical implementation; minimal accuracy drop; interpretable representations; works with transformers","weaknesses":"Limited theoretical analysis; interpretability gains not quantified; computational overhead not analyzed","citation":"Tamkin, A., Taufeeque, M. (2023). Codebook Features: Sparse and Discrete Interpretability for Neural Networks. Stanford AI Lab Blog","notes":"Shows VQ can maintain distributional properties while improving interpretability - relevant to verifiable distribution enforcement","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"bosman2023","title":"Robustness Distributions in Neural Network Verification","authors":"Annelot W. Bosman, Aaron Berger, Holger H. Hoos, Jan N. van Rijn","journal":"JAIR","year":"2023","doi":"","url":"https://www.jair.org/index.php/jair/article/view/18403","keyAssumptions":"Binary robust/non-robust classification sufficient; individual input analysis adequate","keyHypotheses":"Critical epsilon distributions follow log-normal distribution; K-S tests can verify distributional properties of robustness","strengths":"Rigorous statistical analysis; K-S test validation; extensive empirical evaluation; novel robustness distribution perspective","weaknesses":"Limited to MNIST; computational cost of complete verification; theoretical analysis incomplete","citation":"Bosman, A.W., et al. (2023). Robustness Distributions in Neural Network Verification. JAIR","notes":"Direct application of K-S testing to neural network analysis - validates our Random Probe approach for verification","addedDate":"2025-08-26T19:46:00.000Z"}
{"id":"song2024","title":"Toward Architecture-Agnostic Local Control of Posterior Collapse in VAEs","authors":"Hyunsoo Song, Seungwhan Kim, Seungkyu Lee","journal":"arXiv","year":"2024","doi":"10.48550/arXiv.2508.12530","url":"https://arxiv.org/abs/2508.12530","keyAssumptions":"Posterior collapse requires architecture-specific solutions; global solutions sufficient","keyHypotheses":"Local posterior collapse concept with Latent Reconstruction loss can control collapse without architectural restrictions","strengths":"Architecture-agnostic solution; novel local collapse definition; empirical validation across datasets","weaknesses":"Limited theoretical analysis of local vs global collapse; computational overhead unclear","citation":"Song, H., Kim, S., Lee, S. (2024). Toward Architecture-Agnostic Local Control of Posterior Collapse in VAEs. arXiv:2508.12530","notes":"Shows local distributional enforcement can work across architectures - supports architecture-agnostic DERP approach","addedDate":"2025-08-28T19:41:00.000Z"}
{"id":"li2024","title":"Simple, unified analysis of Johnson-Lindenstrauss with applications","authors":"Yingru Li","journal":"arXiv","year":"2024","doi":"10.48550/arXiv.2402.10232","url":"https://arxiv.org/abs/2402.10232","keyAssumptions":"JL lemma constructions require complex, specialized analysis; independence assumptions necessary","keyHypotheses":"Unified analysis framework can simplify understanding and remove independence assumptions","strengths":"Simplified framework; removes independence assumption; unified treatment of constructions; enhanced Hanson-Wright inequality","weaknesses":"Primarily theoretical; limited practical applications demonstrated","citation":"Li, Y. (2024). Simple, unified analysis of Johnson-Lindenstrauss with applications. arXiv:2402.10232","notes":"Provides enhanced theoretical foundation for random projections in DERP - supports statistical testing approach","addedDate":"2025-08-28T19:41:00.000Z"}
{"id":"ayyala2020","title":"Covariance matrix testing in high dimension using random projections","authors":"Deepak Nag Ayyala, Santu Ghosh, Daniel F. Linder","journal":"arXiv","year":"2020","doi":"10.48550/arXiv.2011.08282","url":"https://arxiv.org/abs/2011.08282","keyAssumptions":"High-dimensional covariance testing computationally intractable; traditional methods insufficient","keyHypotheses":"Random projections to lower dimensions enable traditional multivariate tests for covariance matrices","strengths":"Practical solution to curse of dimensionality; extensive simulation study; real data validation","weaknesses":"Limited theoretical analysis; projection dimension selection heuristic","citation":"Ayyala, D.N., Ghosh, S., Linder, D.F. (2020). Covariance matrix testing in high dimension using random projections. arXiv:2011.08282","notes":"Direct precedent for high-dimensional statistical testing via random projections - validates DERP methodology","addedDate":"2025-08-28T19:41:00.000Z"}
{"id":"chen2024","title":"Model checking for high dimensional generalized linear models based on random projections","authors":"Wen Chen, Jie Liu, Heng Peng, Falong Tan, Lixing Zhu","journal":"arXiv","year":"2024","doi":"10.48550/arXiv.2412.10721","url":"https://arxiv.org/abs/2412.10721","keyAssumptions":"Model checking in high dimensions requires complex methods; curse of dimensionality unavoidable","keyHypotheses":"Random projections enable efficient model checking with detection rate n^{-1/2}h^{-1/4} independent of dimension","strengths":"Alleviates curse of dimensionality; dimension-independent detection rate; independent projection statistics","weaknesses":"Limited to GLMs; bandwidth selection required; theoretical gaps remain","citation":"Chen, W., et al. (2024). Model checking for high dimensional generalized linear models based on random projections. arXiv:2412.10721","notes":"Shows random projections maintain statistical power in high dimensions - supports dimension-independent DERP testing","addedDate":"2025-08-28T19:41:00.000Z"}
{"id":"seo2024vq","title":"Rate-Adaptive Quantization: A Multi-Rate Codebook Adaptation for Vector Quantization-based Generative Models","authors":"Jiwan Seo, Joonhyuk Kang","journal":"arXiv","year":"2024","doi":"10.48550/arXiv.2405.14222","url":"https://arxiv.org/abs/2405.14222","keyAssumptions":"Single fixed-rate codebooks adequate; retraining required for different rates","keyHypotheses":"Multi-rate adaptation from single baseline enables flexible rate-distortion trade-offs","strengths":"Single system for multiple rates; data-driven adaptation; outperforms fixed baselines","weaknesses":"Codebook management complexity; potential overfitting; computational overhead","citation":"Seo, J., Kang, J. (2024). Rate-Adaptive Quantization: A Multi-Rate Codebook Adaptation for Vector Quantization-based Generative Models. arXiv:2405.14222","notes":"Shows adaptive codebook learning for flexible requirements - relevant to adaptive distribution enforcement","addedDate":"2025-08-28T19:41:00.000Z"}
{"id":"malidarreh2025","title":"Dual Codebook VQ: Enhanced Image Reconstruction with Reduced Codebook Size","authors":"Parisa Boodaghi Malidarreh, Jillur Rahman Saurav, Thuong Le Hoai Pham, Amir Hajighasemi, Anahita Samadi, Saurabh Shrinivas Maydeo, Mohammad Sadegh Nasr, Jacob M. Luber","journal":"arXiv","year":"2025","doi":"10.48550/arXiv.2503.10832","url":"https://arxiv.org/abs/2503.10832","keyAssumptions":"Codebook utilization problems require larger codebooks; deterministic selection sufficient","keyHypotheses":"Dual codebook with global/local partitioning improves utilization while reducing size","strengths":"Compact codebook (512 vs 1024); state-of-the-art reconstruction; no pre-training required","weaknesses":"Increased complexity; computational overhead of dual mechanism","citation":"Malidarreh, P.B., et al. (2025). Dual Codebook VQ: Enhanced Image Reconstruction with Reduced Codebook Size. arXiv:2503.10832","notes":"Shows structured distributional enforcement in VQ can improve efficiency - relevant to principled DERP design","addedDate":"2025-08-28T19:41:00.000Z"}
{"id":"zhang2025vq","title":"Quantize-then-Rectify: Efficient VQ-VAE Training","authors":"Borui Zhang, Qihang Rao, Wenzhao Zheng, Jie Zhou, Jiwen Lu","journal":"arXiv","year":"2025","doi":"10.48550/arXiv.2507.10547","url":"https://arxiv.org/abs/2507.10547","keyAssumptions":"VQ-VAE training requires extensive computational resources; quantization and training must be simultaneous","keyHypotheses":"Pre-trained VAE can be efficiently converted to VQ-VAE by controlling quantization noise within tolerance","strengths":"Massive computational savings (22 hours vs 4.5 days on 32 A100s); competitive reconstruction quality; practical implementation","weaknesses":"Dependent on pre-trained VAE quality; limited to specific tolerance thresholds","citation":"Zhang, B., et al. (2025). Quantize-then-Rectify: Efficient VQ-VAE Training. arXiv:2507.10547","notes":"Shows quantization tolerance approach - relevant to controlling distributional deviations in DERP","addedDate":"2025-08-28T19:41:00.000Z"}
{"id":"shi2025","title":"From Points to Spheres: A Geometric Reinterpretation of Variational Autoencoders","authors":"Songxuan Shi","journal":"arXiv","year":"2025","doi":"10.48550/arXiv.2507.17255","url":"https://arxiv.org/abs/2507.17255","keyAssumptions":"VAE understood primarily through probabilistic inference; deterministic point representations sufficient","keyHypotheses":"Geometric reinterpretation with Gaussian balls provides intuitive understanding of latent space constraints","strengths":"Novel geometric perspective; unified VAE/VQ-VAE understanding; intuitive explanation of KL effects","weaknesses":"Primarily conceptual; limited empirical validation; theoretical connections incomplete","citation":"Shi, S. (2025). From Points to Spheres: A Geometric Reinterpretation of Variational Autoencoders. arXiv:2507.17255","notes":"Provides geometric intuition for distributional constraints - supports geometric understanding of DERP enforcement","addedDate":"2025-08-28T19:41:00.000Z"}
{"id":"boetius2024","title":"Solving Probabilistic Verification Problems of Neural Networks using Branch and Bound","authors":"David Boetius, Stefan Leue, Tobias Sutter","journal":"arXiv","year":"2024","doi":"10.48550/arXiv.2405.17556","url":"https://arxiv.org/abs/2405.17556","keyAssumptions":"Probabilistic verification computationally intractable; existing methods insufficient","keyHypotheses":"Branch and bound with bound propagation can efficiently solve probabilistic verification problems","strengths":"Significant speedup (minutes to seconds); sound and complete; outperforms dedicated algorithms","weaknesses":"Limited to feed-forward networks; scalability questions remain","citation":"Boetius, D., Leue, S., Sutter, T. (2024). Solving Probabilistic Verification Problems of Neural Networks using Branch and Bound. arXiv:2405.17556","notes":"Efficient probabilistic verification approach - supports computational feasibility of DERP verification","addedDate":"2025-08-28T19:41:00.000Z"}